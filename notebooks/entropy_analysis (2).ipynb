{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Comparing diversity of sequence across the Covid MSA to language model uncertainty.**"
      ],
      "metadata": {
        "id": "PNMUijK3ZpNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download coronavirus MSA"
      ],
      "metadata": {
        "id": "bs2UQ77tSA5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the Figshare SARS-CoV-2 MSA dataset\n",
        "!pip install gdown\n",
        "import gdown\n",
        "\n",
        "file_id = \"1RHeifP_M0zpJtf9v7MD5VqXyD27I3YE9\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Google Drive \"shareable link\" format:\n",
        "output = \"1\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "## Or download manually from:\n",
        "# \"https://figshare.com/ndownloader/articles/20486178/versions/1\""
      ],
      "metadata": {
        "id": "He5dNMRRiQYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch biopython matplotlib seaborn scipy pandas tqdm\n"
      ],
      "metadata": {
        "id": "yaixx6qCifud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import spearmanr, pearsonr, linregress\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "from Bio import SeqIO\n",
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import scipy.stats #import entropy\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ],
      "metadata": {
        "id": "3zuk_aamiYGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(max_sequences=100):\n",
        "    \"\"\"Load aligned genome sequences\"\"\"\n",
        "    if os.path.exists('1'):\n",
        "        with zipfile.ZipFile('1', 'r') as zip_ref:\n",
        "            zip_ref.extractall('sars_cov2_data')\n",
        "\n",
        "    sequences = []\n",
        "    msa_file = 'sars_cov2_data/MSA.fasta'\n",
        "\n",
        "    for i, record in enumerate(SeqIO.parse(msa_file, \"fasta\")):\n",
        "        sequences.append(str(record.seq))\n",
        "        if i >= max_sequences - 1:\n",
        "            break\n",
        "\n",
        "    print(f\"Loaded {len(sequences)} aligned genome sequences of length {len(sequences[0])}\")\n",
        "    return sequences"
      ],
      "metadata": {
        "id": "DvzejTl2ibtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = load_data(max_sequences=1)"
      ],
      "metadata": {
        "id": "O7DBVxq3ikEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fit nucleotide transformer model to a single sequence"
      ],
      "metadata": {
        "id": "8AOoH7LXSH5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and tokenizer\n",
        "model_name = \"InstaDeepAI/nucleotide-transformer-500m-human-ref\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Reset CUDA and load model safely\n",
        "torch.cuda.empty_cache()\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
        "model = model.cuda()\n",
        "\n",
        "sequence = a[0].upper()\n",
        "\n",
        "# Process sequence in chunks to avoid memory issues\n",
        "chunk_size = 500  # Reduced chunk size\n",
        "results = []\n",
        "\n",
        "for start_idx in range(0, len(sequence), chunk_size):\n",
        "    chunk = sequence[start_idx:start_idx + chunk_size]\n",
        "    tokens = tokenizer(chunk, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)  # Reduced max_length\n",
        "    input_ids = tokens['input_ids'].cuda()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**{k: v.cuda() for k, v in tokens.items()})\n",
        "        probs = torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "    decoded_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
        "\n",
        "    for i, (token_id, token_str) in enumerate(zip(input_ids[0], decoded_tokens)):\n",
        "        if token_str not in ['<cls>', '<sep>', '<pad>', '<eos>', '<unk>'] and token_id.item() < tokenizer.vocab_size:\n",
        "            # Calculate position more accurately\n",
        "            actual_pos = start_idx + max(0, i-1) * 6  # Assuming 6-mer tokens\n",
        "            end_pos = min(actual_pos + 6, len(sequence))\n",
        "\n",
        "            token_probs = probs[0, i].cpu().numpy()\n",
        "\n",
        "            # Only get valid token probabilities (avoid out-of-bounds)\n",
        "            valid_indices = min(len(token_probs), tokenizer.vocab_size)\n",
        "            result_row = {\n",
        "                'position': f\"{actual_pos}:{end_pos}\",\n",
        "                'actual_token': token_str,\n",
        "                'actual_prob': float(token_probs[token_id.item()])\n",
        "            }\n",
        "\n",
        "            # Only add probabilities for valid tokens\n",
        "            for j in range(valid_indices):\n",
        "                try:\n",
        "                    token_name = tokenizer.convert_ids_to_tokens([j])[0]\n",
        "                    result_row[f'prob_{token_name}'] = float(token_probs[j])\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            results.append(result_row)\n",
        "\n",
        "    # Clear GPU memory\n",
        "    del outputs, probs\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('nucleotide_predictions_all.csv', index=False)\n",
        "\n",
        "actual_probs = df['actual_prob'].values\n",
        "print(f\"Mean probability of actual tokens: {actual_probs.mean():.4f}\")\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"Sample rows:\\n{df.head()}\")"
      ],
      "metadata": {
        "id": "4rq_ExoHmip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate the language model entropy values at each token"
      ],
      "metadata": {
        "id": "T20LOcIoSONC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the predictions CSV\n",
        "df = pd.read_csv('nucleotide_predictions_all.csv')\n",
        "\n",
        "# Get all probability columns (excluding metadata)\n",
        "prob_cols = [col for col in df.columns if col.startswith('prob_')]\n",
        "\n",
        "# Calculate entropy for each token\n",
        "entropies = []\n",
        "for idx, row in df.iterrows():\n",
        "    # Get probability values for this token\n",
        "    probs = [row[col] for col in prob_cols]\n",
        "    probs = np.array(probs)\n",
        "\n",
        "    # Remove zeros and normalize to avoid log(0)\n",
        "    probs = probs[probs > 0]\n",
        "    probs = probs / probs.sum()  # Normalize\n",
        "\n",
        "    # Calculate Shannon entropy: H = -sum(p * log2(p))\n",
        "    entropy = -np.sum(probs * np.log2(probs))\n",
        "    entropies.append(entropy)\n",
        "\n",
        "# Parse position ranges to get start and end indices\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "for pos in df['position']:\n",
        "    start, end = map(int, pos.split(':'))\n",
        "    start_positions.append(start)\n",
        "    end_positions.append(end)\n",
        "\n",
        "# Create entropy dataframe\n",
        "entropy_df = pd.DataFrame({\n",
        "    'start_position': start_positions,\n",
        "    'end_position': end_positions,\n",
        "    'token': df['actual_token'],\n",
        "    'entropy': entropies,\n",
        "    'actual_token_prob': df['actual_prob']\n",
        "})\n",
        "\n",
        "# Save entropy results\n",
        "entropy_df.to_csv('token_entropy.csv', index=False)\n",
        "\n",
        "print(f\"Entropy statistics:\")\n",
        "print(f\"Mean entropy: {np.mean(entropies):.4f}\")\n",
        "print(f\"Min entropy: {np.min(entropies):.4f}\")\n",
        "print(f\"Max entropy: {np.max(entropies):.4f}\")\n",
        "print(f\"Std entropy: {np.std(entropies):.4f}\")\n",
        "print(f\"\\nSample results:\")\n",
        "print(entropy_df.head(10))"
      ],
      "metadata": {
        "id": "V4jYWqu8rIJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate MSA entropies for each token."
      ],
      "metadata": {
        "id": "kzRN-TxzSoWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy_chunks_data(positions_csv='positions.csv', max_sequences=100):\n",
        "    \"\"\"Load aligned genome sequences and calculate entropy for specified nucleotide chunks\"\"\"\n",
        "    if os.path.exists('1'):\n",
        "        with zipfile.ZipFile('1', 'r') as zip_ref:\n",
        "            zip_ref.extractall('sars_cov2_data')\n",
        "\n",
        "    # Load position ranges\n",
        "    positions_df = pd.read_csv(positions_csv)\n",
        "    msa_file = 'sars_cov2_data/MSA.fasta'\n",
        "\n",
        "    # Load all sequences (needed for chunk comparison)\n",
        "    sequences = []\n",
        "    for i, record in enumerate(SeqIO.parse(msa_file, \"fasta\")):\n",
        "        if i >= max_sequences:\n",
        "            break\n",
        "        sequences.append(str(record.seq))\n",
        "\n",
        "    # Calculate entropy for each chunk\n",
        "    results = []\n",
        "    for _, row in positions_df.iterrows():\n",
        "        start, end = int(row['start_position']), int(row['end_position'])\n",
        "\n",
        "        # Extract chunks from all sequences\n",
        "        chunks = [seq[start:end] for seq in sequences]\n",
        "\n",
        "        # Count chunk frequencies and calculate entropy\n",
        "        chunk_counts = list(Counter(chunks).values())\n",
        "        chunk_entropy = scipy.stats.entropy(chunk_counts)\n",
        "\n",
        "        results.append({\n",
        "            'start_position': start,\n",
        "            'end_position': end,\n",
        "            'chunk_length': len(chunks[0]),\n",
        "            'entropy': chunk_entropy\n",
        "        })\n",
        "\n",
        "    # Save to CSV\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv('chunk_entropy.csv', index=False)\n",
        "\n",
        "    print(f\"Calculated chunk entropy for {len(sequences)} sequences\")\n",
        "    print(f\"Processed {len(results)} chunks, saved to chunk_entropy.csv\")\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "pXbupEKyrFUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entropy_chunks_data(positions_csv='token_entropy.csv', max_sequences=100000)"
      ],
      "metadata": {
        "id": "-lHOW325xLxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot LM diagnostics"
      ],
      "metadata": {
        "id": "7fDvOnajS8aD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the predictions and entropy CSVs\n",
        "df = pd.read_csv('nucleotide_predictions_all.csv')\n",
        "entropy_df = pd.read_csv('token_entropy.csv')\n",
        "\n",
        "# Parse positions for plotting\n",
        "positions = []\n",
        "for pos in df['position']:\n",
        "    start, end = map(int, pos.split(':'))\n",
        "    positions.append(start)\n",
        "df['start_pos'] = positions\n",
        "\n",
        "# Merge dataframes using start_pos\n",
        "df_merged = df.merge(entropy_df[['start_position', 'entropy']], left_on='start_pos', right_on='start_position', how='left')\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "\n",
        "# Plot 1: Actual token probability distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['actual_prob'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Actual Token Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Actual Token Probabilities')\n",
        "plt.axvline(df['actual_prob'].mean(), color='red', linestyle='--', label=f'Mean: {df[\"actual_prob\"].mean():.3f}')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot1_probability_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot 2: Entropy distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(entropy_df['entropy'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "plt.xlabel('Shannon Entropy')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Prediction Entropy')\n",
        "plt.axvline(entropy_df['entropy'].mean(), color='red', linestyle='--', label=f'Mean: {entropy_df[\"entropy\"].mean():.3f}')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot2_entropy_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot 3: Probability vs Position (sliding window)\n",
        "plt.figure(figsize=(12, 6))\n",
        "window_size = 100\n",
        "if len(df_merged) > window_size:\n",
        "    windowed_probs = [df_merged['actual_prob'].iloc[i:i+window_size].mean()\n",
        "                     for i in range(0, len(df_merged)-window_size, window_size//4)]\n",
        "    windowed_positions = [df_merged['start_pos'].iloc[i:i+window_size].mean()\n",
        "                         for i in range(0, len(df_merged)-window_size, window_size//4)]\n",
        "    plt.plot(windowed_positions, windowed_probs, linewidth=2, color='purple')\n",
        "else:\n",
        "    plt.scatter(df_merged['start_pos'], df_merged['actual_prob'], alpha=0.6, color='purple')\n",
        "plt.xlabel('Sequence Position')\n",
        "plt.ylabel('Average Actual Token Probability')\n",
        "plt.title('Prediction Quality Along Sequence')\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot3_quality_along_sequence.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot 4: Entropy vs Position\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(entropy_df['start_position'], entropy_df['entropy'], alpha=0.6, color='orange', s=10)\n",
        "plt.xlabel('Sequence Position')\n",
        "plt.ylabel('Shannon Entropy')\n",
        "plt.title('Prediction Uncertainty Along Sequence')\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot4_entropy_along_sequence.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot 5: Probability vs Entropy scatter\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(entropy_df['actual_token_prob'], entropy_df['entropy'], alpha=0.6, color='red', s=10)\n",
        "plt.xlabel('Actual Token Probability')\n",
        "plt.ylabel('Shannon Entropy')\n",
        "plt.title('Probability vs Uncertainty')\n",
        "corr = stats.pearsonr(entropy_df['actual_token_prob'], entropy_df['entropy'])[0]\n",
        "plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes,\n",
        "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot5_probability_vs_entropy.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Plot 6: Top vs bottom performing tokens\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_tokens = df.nlargest(20, 'actual_prob')['actual_token'].value_counts()\n",
        "bottom_tokens = df.nsmallest(20, 'actual_prob')['actual_token'].value_counts()\n",
        "\n",
        "# Get union of all tokens and reindex both series\n",
        "all_tokens = list(set(top_tokens.index) | set(bottom_tokens.index))\n",
        "top_reindexed = top_tokens.reindex(all_tokens, fill_value=0)\n",
        "bottom_reindexed = bottom_tokens.reindex(all_tokens, fill_value=0)\n",
        "\n",
        "token_performance = pd.DataFrame({\n",
        "    'high_conf': top_reindexed,\n",
        "    'low_conf': bottom_reindexed\n",
        "})\n",
        "\n",
        "token_performance.plot(kind='bar', color=['green', 'red'], alpha=0.7)\n",
        "plt.xlabel('Token')\n",
        "plt.ylabel('Count')\n",
        "plt.title('High vs Low Confidence Token Types')\n",
        "plt.legend(['High Confidence', 'Low Confidence'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('plot6_token_performance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"=== LANGUAGE MODEL PERFORMANCE SUMMARY ===\")\n",
        "print(f\"Total tokens analyzed: {len(df)}\")\n",
        "print(f\"Mean actual token probability: {df['actual_prob'].mean():.4f}\")\n",
        "print(f\"Median actual token probability: {df['actual_prob'].median():.4f}\")\n",
        "print(f\"Mean entropy: {entropy_df['entropy'].mean():.4f}\")\n",
        "print(f\"Tokens with >50% probability: {(df['actual_prob'] > 0.5).sum()} ({(df['actual_prob'] > 0.5).mean()*100:.1f}%)\")\n",
        "print(f\"Tokens with >90% probability: {(df['actual_prob'] > 0.9).sum()} ({(df['actual_prob'] > 0.9).mean()*100:.1f}%)\")\n",
        "print(f\"Probability-Entropy correlation: {stats.pearsonr(entropy_df['actual_token_prob'], entropy_df['entropy'])[0]:.4f}\")\n",
        "\n",
        "# Save summary stats\n",
        "summary_stats = {\n",
        "    'total_tokens': len(df),\n",
        "    'mean_prob': df['actual_prob'].mean(),\n",
        "    'median_prob': df['actual_prob'].median(),\n",
        "    'mean_entropy': entropy_df['entropy'].mean(),\n",
        "    'high_conf_tokens_50pct': (df['actual_prob'] > 0.5).sum(),\n",
        "    'high_conf_tokens_90pct': (df['actual_prob'] > 0.9).sum(),\n",
        "    'prob_entropy_correlation': stats.pearsonr(entropy_df['actual_token_prob'], entropy_df['entropy'])[0]\n",
        "}\n",
        "\n",
        "pd.DataFrame([summary_stats]).to_csv('lm_performance_summary.csv', index=False)\n",
        "print(f\"\\nPlots saved as separate PNG files and summary to 'lm_performance_summary.csv'\")"
      ],
      "metadata": {
        "id": "GViv16zXthxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot regression of LM vs MSA entropies, and find correlations between non-overlapping windows."
      ],
      "metadata": {
        "id": "VswaQpfxUIgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "output_folder = 'entropy_output_2'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "msa = pd.read_csv('chunk_entropy.csv')\n",
        "lm = pd.read_csv('token_entropy.csv')\n",
        "\n",
        "# Check positions match\n",
        "print(\"Position match:\", set(zip(msa.start_position, msa.end_position)) == set(zip(lm.start_position, lm.end_position)))\n",
        "\n",
        "# Merge data\n",
        "df = pd.merge(msa, lm, on=['start_position', 'end_position'], suffixes=('_msa', '_lm'))\n",
        "\n",
        "# 1. Scatter plot with regression\n",
        "plt.figure(figsize=(8, 6))\n",
        "x, y = df.entropy_msa, df.entropy_lm\n",
        "slope, intercept, r_value, _, _ = linregress(x, y)\n",
        "plt.scatter(x, y, alpha=0.6)\n",
        "plt.plot(x, slope*x + intercept, 'r-', linewidth=2)\n",
        "plt.xlabel('MSA Entropy')\n",
        "plt.ylabel('LM Entropy')\n",
        "plt.title(f'MSA vs LM Entropy (R² = {r_value**2:.3f})')\n",
        "plt.savefig(f'{output_folder}/scatter_regression.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# 2. Moving averages and correlations for different window sizes\n",
        "windows = [2,5,10,20,30,40, 50, 100, 500, 1000]\n",
        "correlations = {}\n",
        "divergent_regions = {}\n",
        "nonoverlap_results = {}\n",
        "\n",
        "for w in windows:\n",
        "    # Calculate moving averages (overlapping) - for visualization only\n",
        "    msa_ma = df.entropy_msa.rolling(w, center=True).mean()\n",
        "    lm_ma = df.entropy_lm.rolling(w, center=True).mean()\n",
        "\n",
        "    # Calculate correlation for overlapping windows (biased, for visualization)\n",
        "    valid = ~(msa_ma.isna() | lm_ma.isna())\n",
        "    corr_overlap, p_overlap = pearsonr(msa_ma[valid], lm_ma[valid])\n",
        "    correlations[w] = corr_overlap\n",
        "\n",
        "    # Non-overlapping windows analysis (UNBIASED for statistics)\n",
        "    n_windows = len(df) // w\n",
        "    msa_nonoverlap = []\n",
        "    lm_nonoverlap = []\n",
        "\n",
        "    for i in range(n_windows):\n",
        "        start_idx = i * w\n",
        "        end_idx = start_idx + w\n",
        "        msa_nonoverlap.append(df.entropy_msa.iloc[start_idx:end_idx].mean())\n",
        "        lm_nonoverlap.append(df.entropy_lm.iloc[start_idx:end_idx].mean())\n",
        "\n",
        "    msa_nonoverlap = np.array(msa_nonoverlap)\n",
        "    lm_nonoverlap = np.array(lm_nonoverlap)\n",
        "\n",
        "    # TRUE correlation and significance (non-overlapping)\n",
        "    corr_true, p_true = pearsonr(msa_nonoverlap, lm_nonoverlap)\n",
        "    sig_threshold = 1.96 / np.sqrt(n_windows - 3) if n_windows > 3 else np.nan\n",
        "    is_significant = abs(corr_true) > sig_threshold if not np.isnan(sig_threshold) else False\n",
        "\n",
        "    nonoverlap_results[w] = {\n",
        "        'correlation': corr_true,\n",
        "        'p_value': p_true,\n",
        "        'n_windows': n_windows,\n",
        "        'threshold': sig_threshold,\n",
        "        'is_significant': is_significant\n",
        "    }\n",
        "\n",
        "    # Scatter plot for non-overlapping windows\n",
        "    slope_no, intercept_no, r_no, p_no, _ = linregress(msa_nonoverlap, lm_nonoverlap)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(msa_nonoverlap, lm_nonoverlap, alpha=0.7, s=50)\n",
        "    plt.plot(msa_nonoverlap, slope_no*msa_nonoverlap + intercept_no, 'r-', linewidth=2)\n",
        "    plt.xlabel('MSA Entropy (averaged)')\n",
        "    plt.ylabel('LM Entropy (averaged)')\n",
        "    sig_text = \"***\" if p_true < 0.001 else \"**\" if p_true < 0.01 else \"*\" if p_true < 0.05 else \"ns\"\n",
        "    plt.title(f'Non-overlapping windows (size={w})\\nR² = {r_no**2:.3f}, n = {n_windows}, p = {p_true:.3e} {sig_text}')\n",
        "    plt.savefig(f'{output_folder}/scatter_nonoverlap_w{w}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Find divergent regions (top 5% of absolute differences)\n",
        "    diff = np.abs(msa_ma - lm_ma)\n",
        "    threshold_div = diff.quantile(0.95)\n",
        "    divergent_idx = diff > threshold_div\n",
        "    divergent_regions[w] = df.loc[divergent_idx, ['start_position', 'end_position']].values\n",
        "\n",
        "    # Plot separate traces\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(msa_ma, label='MSA', alpha=0.7)\n",
        "    plt.plot(lm_ma, label='LM', alpha=0.7)\n",
        "    plt.title(f'Separate traces (window={w})')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot normalized together\n",
        "    plt.subplot(1, 2, 2)\n",
        "    msa_norm = (msa_ma - msa_ma.min()) / (msa_ma.max() - msa_ma.min())\n",
        "    lm_norm = (lm_ma - lm_ma.min()) / (lm_ma.max() - lm_ma.min())\n",
        "    plt.plot(msa_norm, label='MSA (normalized)', alpha=0.7)\n",
        "    plt.plot(lm_norm, label='LM (normalized)', alpha=0.7)\n",
        "    plt.title(f'Normalized traces (window={w})\\nOverlapping r={corr_overlap:.3f} | Independent r={corr_true:.3f} {sig_text}')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{output_folder}/traces_window_{w}.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Window {w}: Overlapping r={corr_overlap:.3f} (BIASED) | Independent r={corr_true:.3f} (p={p_true:.3e}) n={n_windows} {sig_text}\")\n",
        "\n",
        "# Summary plot of correlations vs window size\n",
        "plt.figure(figsize=(10, 6))\n",
        "overlap_corrs = list(correlations.values())\n",
        "indep_corrs = [nonoverlap_results[w]['correlation'] for w in windows]\n",
        "\n",
        "plt.plot(windows, overlap_corrs, 'o-', linewidth=2, markersize=8, label='Overlapping (biased)', alpha=0.7)\n",
        "plt.plot(windows, indep_corrs, 's-', linewidth=2, markersize=8, label='Independent (unbiased)')\n",
        "plt.xlabel('Window Size')\n",
        "plt.ylabel('Correlation')\n",
        "plt.title('Correlation vs Window Size')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.savefig(f'{output_folder}/correlation_vs_window.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Save results\n",
        "results = pd.DataFrame({\n",
        "    'window_size': windows,\n",
        "    'overlapping_correlation': [correlations[w] for w in windows],\n",
        "    'independent_correlation': [nonoverlap_results[w]['correlation'] for w in windows],\n",
        "    'p_value': [nonoverlap_results[w]['p_value'] for w in windows],\n",
        "    'n_windows': [nonoverlap_results[w]['n_windows'] for w in windows],\n",
        "    'significance_threshold': [nonoverlap_results[w]['threshold'] for w in windows],\n",
        "    'is_significant': [nonoverlap_results[w]['is_significant'] for w in windows],\n",
        "    'num_divergent_regions': [len(divergent_regions[w]) for w in windows]\n",
        "})\n",
        "results.to_csv(f'{output_folder}/analysis_summary.csv', index=False)\n",
        "\n",
        "print(f\"\\nAnalysis complete! Results saved in '{output_folder}/' folder\")\n",
        "print(f\"Overall correlation: {pearsonr(df.entropy_msa, df.entropy_lm)[0]:.3f}\")\n",
        "print(\"\\nTrue Significance Summary (Non-overlapping):\")\n",
        "print(results[['window_size', 'independent_correlation', 'p_value', 'n_windows', 'is_significant']])\n",
        "\n",
        "print(f\"\\nWhy p-values were misleading:\")\n",
        "print(\"- Overlapping windows violate independence assumption\")\n",
        "print(\"- Each rolling average shares most data with neighbors\")\n",
        "print(\"- This inflates sample size and deflates p-values artificially\")\n",
        "print(\"- Non-overlapping analysis gives TRUE statistical significance\")"
      ],
      "metadata": {
        "id": "YlFsfd9V53sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find regions enriched for low MSA, high LM entropy."
      ],
      "metadata": {
        "id": "mZvLf0yFTb5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify outliers: Low LM entropy + High MSA entropy\n",
        "lm_threshold = np.percentile(df.entropy_lm, 75)\n",
        "msa_threshold = np.percentile(df.entropy_msa, 25)\n",
        "outliers = df[(df.entropy_lm >= lm_threshold) & (df.entropy_msa <= msa_threshold)]\n",
        "# outliers = df[(df.entropy_msa <= msa_threshold)]\n",
        "output_folder = 'low_msa_high_lm'\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# print(f\"Found {len(outliers)} outliers (Low LM + High MSA)\")\n",
        "\n",
        "# Plot analysis\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "# 1. Scatter with outliers highlighted\n",
        "ax1.scatter(df.entropy_msa, df.entropy_lm, alpha=0.5, s=15, color='lightblue')\n",
        "ax1.scatter(outliers.entropy_msa, outliers.entropy_lm, s=30, color='red', label=f'n={len(outliers)}')\n",
        "ax1.axhline(lm_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "ax1.axvline(msa_threshold, color='red', linestyle='--', alpha=0.7)\n",
        "ax1.set_xlabel('MSA Entropy')\n",
        "ax1.set_ylabel('LM Entropy')\n",
        "ax1.set_title('Outliers: Low MSA, High LM')\n",
        "ax1.legend()\n",
        "\n",
        "# 2. Genomic distribution\n",
        "outlier_pos = (outliers.start_position + outliers.end_position) / 2\n",
        "ax2.hist(outlier_pos, bins=30, alpha=0.7, color='red')\n",
        "ax2.set_xlabel('Genomic Position')\n",
        "ax2.set_ylabel('Count')\n",
        "ax2.set_title('Outlier Distribution')\n",
        "\n",
        "# 3. Enrichment across genome segments\n",
        "n_seg = 20\n",
        "seg_size = df.end_position.max() / n_seg\n",
        "expected_rate = len(outliers) / len(df)\n",
        "\n",
        "enrichments = []\n",
        "p_vals = []\n",
        "for i in range(n_seg):\n",
        "    start, end = i * seg_size, (i+1) * seg_size\n",
        "    seg_outliers = len(outliers[(outlier_pos >= start) & (outlier_pos < end)])\n",
        "    seg_total = len(df[((df.start_position + df.end_position)/2 >= start) &\n",
        "                      ((df.start_position + df.end_position)/2 < end)])\n",
        "\n",
        "    if seg_total > 0:\n",
        "        obs_rate = seg_outliers / seg_total\n",
        "        enrichment = obs_rate / expected_rate if expected_rate > 0 else 0\n",
        "        p_val = stats.binomtest(seg_outliers, seg_total, expected_rate, alternative='greater')\n",
        "    else:\n",
        "        enrichment, p_val = 0, 1\n",
        "\n",
        "    enrichments.append(enrichment)\n",
        "    p_vals.append(p_val)\n",
        "\n",
        "colors = ['red' if p.pvalue < 0.05 else 'blue' for p in p_vals]\n",
        "seg_centers = [(i + 0.5) * seg_size for i in range(n_seg)]\n",
        "ax3.bar(seg_centers, enrichments, width=seg_size*0.8, color=colors, alpha=0.7)\n",
        "ax3.axhline(1, color='black', linestyle='-', alpha=0.5)\n",
        "ax3.set_xlabel('Genomic Position')\n",
        "ax3.set_ylabel('Enrichment')\n",
        "ax3.set_title('Enrichment (Red=p<0.05)')\n",
        "\n",
        "# 4. Cumulative distribution\n",
        "all_pos = (df.start_position + df.end_position) / 2\n",
        "ks_stat, ks_p = stats.ks_2samp(all_pos, outlier_pos)\n",
        "ax4.hist(all_pos, bins=50, alpha=0.5, density=True, cumulative=True, label='All')\n",
        "ax4.hist(outlier_pos, bins=20, alpha=0.7, density=True, cumulative=True, label='Outliers', color='red')\n",
        "ax4.set_xlabel('Genomic Position')\n",
        "ax4.set_ylabel('Cumulative Density')\n",
        "ax4.set_title(f'Distribution (KS p={ks_p:.2e})')\n",
        "ax4.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{output_folder}/outlier_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "# Save results\n",
        "outliers.to_csv(f'{output_folder}/outliers.csv', index=False)\n",
        "print(f\"KS test p-value: {ks_p:.2e}\")\n",
        "sig_enriched = [(i, enrichments[i]) for i, p in enumerate(p_vals) if p.pvalue < 0.05 and enrichments[i] > 1.5]\n",
        "if sig_enriched:\n",
        "    print(\"Enriched regions:\", [(f\"{i*seg_size:.0f}-{(i+1)*seg_size:.0f}\", f\"{e:.1f}x\") for i, e in sig_enriched])\n",
        "else:\n",
        "    print(\"No significant enrichment\")"
      ],
      "metadata": {
        "id": "nxASbej8-a23"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}